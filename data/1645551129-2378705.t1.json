{"response": "{\n  \"links\": [\n    \"http://algomus.fr/data\",\n    \"https://gitlab.com/algomus.fr/algomus-data/tree/master/fugues\",\n    \"https://gitlab.com/algomus.fr/algomus-data/tree/master/quartets/texture\"\n  ]\n}", "webpage": "http://algomus.fr/data/", "htmlcode": "<!DOCTYPE HTML>\n\n<!--\n  Stellar by HTML5 UP\n  html5up.net | @ajlkn\n  Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)\n-->\n<html>\n\n<body>\n<div id=\"wrapper\">\n<nav id=\"nav\">\n<ul>\n<li><a class=\"active\" href=\"http://algomus.fr/\"><b>Algomus</b></a></li>\n<li><a class=\"active\" href=\"http://algomus.fr/data\">Data</a></li>\n<li><a class=\"active\" href=\"https://www.cristal.univ-lille.fr/equipes/algomus/publications/\">Publications \u00f0\u009f\u0094\u0097</a></li>\n<li><a class=\"active\" href=\"http://algomus.fr/before-you-fly\">AI Song 2021</a></li>\n<li><a class=\"active\" href=\"http://algomus.fr/sim\">Events</a></li>\n<li><a class=\"active\" href=\"http://algomus.fr/tabasco\">TABASCO</a></li>\n<li><a class=\"active\" href=\"http://algomus.fr/dezrann\">Dezrann</a></li>\n<li><a class=\"active\" href=\"http://algomus.fr/jobs\">Jobs</a></li>\n</ul>\n</nav>\n<div id=\"main\">\n<section class=\"main\">\n<h1 id=\"algomus-datasets\">Algomus datasets</h1>\n<p><a href=\"http://www.algomus.fr/data\">http://www.algomus.fr/data</a></p>\n<p>This repository contains and links public data released by the <a href=\"http://www.algomus.fr\">Algomus</a> team.\nOpen data and corpora are essential for digital humanities studies.\nSystematic musicology approaches use corpora to infer new music knowledge or to confirm or challenge existing theories.\nComputational music analysis (CMA), or, more generally, music information retrieval (MIR) studies design algorithms that need to be evaluated.</p>\n<p>Building corpora with reference analyses is not so easy \u00e2\u0080\u0093 there are many different analyses of the same piece that are musically relevant. However, some analysis are definitely more correct than others. Some reference annotations can be used to evaluate some parts of analysis algorithms.\nAs computer or data scientists, we would like to have computer-readable reference datasets that may be used as a ground truth to evaluate MIR/CMA algorithms or to infer new music\nknowledge. But as music theorists, we know that there is not only one correct analysis of a given piece: listeners, players, or theorists often disagree or at least propose several points of view. Anyway, there is consensus about some analytical elements by many music theorists, players or listeners. The fact that reaching consensus may be difficult on some points should not prevent us from trying to formalize some elements.</p>\n<h2 id=\"available-datasets\">Available datasets</h2>\n<hr/>\n<h3 id=\"fugues\">Fugues</h3>\n<p>Data: <a href=\"https://gitlab.com/algomus.fr/algomus-data/tree/master/fugues\">https://gitlab.com/algomus.fr/algomus-data/tree/master/fugues</a></p>\n<ul>\n<li>24 Bach fugues (WTC I, BWV 846-893) + 12 Shostakovich fugues (op.57, 1952)</li>\n<li>S/CS/CS2 patterns, cadences, pedals (1000+ labels)</li>\n<li><code>.ref</code> (2013 \u2013 2016) + converted <code>.dez</code> (2019)</li>\n</ul>\n<p>Reference: M. Giraud et al., Computational Fugue Analysis, Computer Music Journal, 39(2), 77-96, doi:10.1162/COMJ_a_00300, 2015, <a href=\"https://hal.archives-ouvertes.fr/hal-01113520\">https://hal.archives-ouvertes.fr/hal-01113520</a></p>\n<p>See also <a href=\"http://www.algomus.fr/fugues\">http://www.algomus.fr/fugues</a></p>\n<hr/>\n<h3 id=\"texture-in-string-quartets\">Texture in string quartets</h3>\n<p>Data: <a href=\"https://gitlab.com/algomus.fr/algomus-data/tree/master/quartets/texture\">https://gitlab.com/algomus.fr/algomus-data/tree/master/quartets/texture</a></p>\n<ul>\n<li>11 movements (Haydn, Mozart, Schubert)</li>\n<li>700+ texture labels</li>\n<li><code>.ref</code> (2014 and 2021), also translated from <code>.dez</code> files (2021)</li>\n</ul>\n<p>See also <a href=\"http://www.algomus.fr/texture\">http://www.algomus.fr/texture</a></p>\n<p>References:</p>\n<ul>\n<li>M. Giraud et al., <a href=\"https://hal.archives-ouvertes.fr/hal-01057017\">Towards modeling texture in symbolic data</a>, ISMIR 2014</li>\n<li>L. Soum-Fontez et al., <a href=\"https://hal.archives-ouvertes.fr/hal-03322543\">Symbolic Textural Features and Melody/Accompaniment Detection in String Quartets</a>, CMMR 2021</li>\n</ul>\n<hr/>\n<h3 id=\"texture-in-piano-music\">Texture in piano music</h3>\n<p>Data: <a href=\"https://gitlab.com/algomus.fr/symbolic-texture-dataset\">https://gitlab.com/algomus.fr/symbolic-texture-dataset</a></p>\n<ul>\n<li>9 movements from Mozart piano sonatas (K279, K280 and K283)</li>\n<li>1164 texture labels</li>\n<li><code>.dez</code>, <code>.txt</code> (human-readable format), <code>.tsv</code> (machine-readable format)</li>\n</ul>\n<p>References:</p>\n<ul>\n<li>L. Couturier et al., <a href=\"https://hal.archives-ouvertes.fr/hal-03860195\">A Dataset of Symbolic Texture Annotations in Mozart Piano Sonatas</a>, ISMIR 2022</li>\n<li>L. Couturier et al., <a href=\"https://hal.archives-ouvertes.fr/hal-03631151\">Annotating Symbolic Texture in Piano Music: a Formal Syntax</a>, SMC 2022</li>\n</ul>\n<hr/>\n<h3 id=\"mozart-string-quartets\">Mozart String Quartets</h3>\n<p>Data: <a href=\"https://gitlab.com/algomus.fr/algomus-data/tree/master/quartets/mozart\">https://gitlab.com/algomus.fr/algomus-data/tree/master/quartets/mozart</a></p>\n<ul>\n<li>32 sonata-form and sonata-form-like movements</li>\n<li>sonata form structure (following Hepokoski&amp;Darcy 2006) and cadences (2000+ labels)</li>\n<li><code>.dez</code> encoded within Dezrann by L. Feisthauer (2017 \u2013 2019)</li>\n</ul>\n<p>Additional data:\n<a href=\"http://algomus.fr/sonata/mozart-quartets-features.zip\">features occurrences</a>,\n<a href=\"http://algomus.fr/sonata/mozart-quartets-models.txt\">models with learned probabilities</a>\n(2019)</p>\n<p>Reference: P. Allegraud et al., Learning sonata form structure on Mozart\u2019s string quartets, in revision</p>\n<p>See also <a href=\"http://www.algomus.fr/sonata\">http://www.algomus.fr/sonata</a></p>\n<hr/>\n<h3 id=\"orchestration\">Orchestration</h3>\n<p>Data: <a href=\"https://gitlab.com/algomus.fr/orchestration\">https://gitlab.com/algomus.fr/orchestration</a></p>\n<ul>\n<li>24 first movements of Haydn, Mozart, and Beethoven symphonies</li>\n<li>7941 texture labels describing 8528 bars</li>\n</ul>\n<p>Reference: D. V. T. Le et al.,\n<a href=\"https://dx.doi.org/10.1145/3543882.3543884\">A Corpus Describing Orchestral Texture in First Movements of Classical and Early-Romantic Symphonies</a>, DLfM 2022</p>\n<hr/>\n<h3 id=\"guitar-positions\">Guitar positions</h3>\n<p>Data: <a href=\"https://gitlab.com/algomus.fr/algomus-data/tree/master/guitar/\">https://gitlab.com/algomus.fr/algomus-data/tree/master/guitar/</a></p>\n<ul>\n<li>2248 tracks in 1022 songs/pieces</li>\n<li>1000 most frequent position vectors for chords (with 2-3 notes, with 4-6 notes)</li>\n</ul>\n<p>Reference: J. Cournut et al.,\n<a href=\"https://hal.archives-ouvertes.fr/hal-03279863/\">What are the most used guitar positions?</a>, DLfM 2021</p>\n<p>Data: <a href=\"https://gitlab.com/lbigo/rhythm-guitar-detection\">https://gitlab.com/lbigo/rhythm-guitar-detection</a></p>\n<p>For 102 guitar tablatures, at the bar level:</p>\n<ul>\n<li>Manual annotations (rhythm guitar or not)</li>\n<li>31 computed features (note/chord values, variety, playing techniques, etc.)</li>\n</ul>\n<p>Reference: D. Regnier et al.,\n<a href=\"https://hal.archives-ouvertes.fr/hal-03335822/file/regnier-rhythm-guitar.pdf\">Identification of rhythm guitar sections in symbolic tablatures</a>, ISMIR 2021</p>\n<hr/>\n<h2 id=\"data-formats\">Data formats</h2>\n<p>Data are presented in several formats, including:</p>\n<ul>\n<li><code>.dez</code> json files, that can be read and written\n<ul>\n<li>by <a href=\"http://www.dezrann.net\">Dezrann</a></li>\n<li>by our <code>music21.schema</code> module (see <a href=\"https://hal.archives-ouvertes.fr/hal-01135118\">Bagan 2015</a>, upcoming update in 2019)</li>\n</ul>\n</li>\n<li><code>.ref</code> annotation files, described in <a href=\"./syntax.ref\">syntax.ref</a></li>\n</ul>\n<p>We publish all new data in <code>.dez</code> format, and will progressively convert the old files.</p>\n<h2 id=\"license\">License</h2>\n<p>These data are made available under the <a href=\"http://opendatacommons.org/licenses/odbl/1.0/\">Open Database License</a>.\nAny rights in individual contents of the database are licensed under the <a href=\"http://opendatacommons.org/licenses/dbcl/1.0/\">Database Contents License</a>.</p>\n<h2 id=\"other-data\">Other data</h2>\n<h3 id=\"ai-song-contest\">AI Song Contest</h3>\n<p><a href=\"https://gitlab.com/algomus.fr/algomus-data/tree/master/ai-song/2020-i-keep-counting\">https://gitlab.com/algomus.fr/algomus-data/tree/master/ai-song/2020-i-keep-counting</a></p>\n<p>Reference: G. Micchi et al.,\nI Keep Counting: An Experiment in Human/AI Co-creative Songwriting, TISMIR, in press</p>\n<p>See also <a href=\"http://www.algomus.fr/i-keep-counting\">http://www.algomus.fr/i-keep-counting</a></p>\n</section>\n</div>\n<footer id=\"footer\">\n<section>\n<h2>Algomus</h2>\n<p>Can computers help people to understand and enjoy music?</p>\n<ul class=\"actions\">\n</ul>\n</section>\n<section>\n<h2></h2>\n<dl class=\"alt\">\n<dt>Address</dt>\n<dd>CRIStAL, Cit\u00c3\u00a9 Scientifique<br/>59 655 Villeneuve d\u2019Ascq Cedex<br/>France</dd>\n<dt>Email</dt>\n<dd><a href=\"mailto:contact@algomus.fr%20%3cbr%20/%3e%3ca%20href=%27/privacy%27%3ePrivacy%20policy%3c/a%3e\"><a href=\"mailto:contact@algomus.fr\">contact@algomus.fr</a> <br><a href=\"http://algomus.fr/privacy\">Privacy policy</a></br></a></dd>\n</dl>\n<ul class=\"icons\">\n</ul>\n</section>\n</footer>\n</div>\n<!-- Scripts -->\n\n\n\n\n\n<!-- [if lte IE 8 ]]> <script src=\"http://algomus.fr/js/ie/respond.min.js\"></script><![endif] -->\n\n</body>\n</html>"}