{"response": "{\"links\": [\"https://www.dropbox.com/s/o0olz0uwl9z9stb/billboard-2.0-index.csv?dl=1\",\"https://www.dropbox.com/s/p4xtixbvt4hw5c6/billboard-2.0-salami_chords.tar.xz?dl=1\",\"https://www.dropbox.com/s/g9yq0pafbfkbgxr/billboard-2.0-salami_chords.tar.bz2?dl=1\"]}", "webpage": "https://ddmal.music.mcgill.ca/research/The_McGill_Billboard_Project_(Chord_Analysis_Dataset)/", "htmlcode": "<!DOCTYPE html>\n\n<html lang=\"en-us\">\n\n<link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\" rel=\"stylesheet\"/>\n<nav class=\"navbar navbar-light fixed-top navbar-expand-lg bg-white\">\n<a class=\"navbar-brand\" href=\"../../\">DDMAL</a>\n<button aria-controls=\"navbarSupportedContent\" aria-expanded=\"false\" aria-label=\"Toggle navigation\" class=\"navbar-toggler\" data-target=\"#navbarSupportedContent\" data-toggle=\"collapse\" type=\"button\">\n<span class=\"navbar-toggler-icon\"></span>\n</button>\n<div class=\"collapse navbar-collapse\" id=\"navbarSupportedContent\">\n<ul class=\"navbar-nav ml-auto\" id=\"nav-items\">\n<li class=\"nav-item\">\n<a class=\"nav-link\" href=\"../../\">Home <span class=\"sr-only\">(current)</span></a>\n</li>\n<li class=\"nav-item\">\n<a class=\"nav-link\" href=\"../../lab_members/\">Lab Members</a>\n</li>\n<li class=\"dropdown\">\n<a class=\"dropdown-toggle\" data-toggle=\"dropdown\" href=\"#\" role=\"button\">Activities<span class=\"caret\"></span></a>\n<ul class=\"dropdown-menu\" role=\"menu\">\n<li><a href=\"../../activities/media/\" target=\"_top\">Media</a></li>\n<li><a href=\"../../activities/posters/\" target=\"_top\">Posters</a></li>\n<li><a href=\"../../activities/presentations/\" target=\"_top\">Presentations</a></li>\n<li><a href=\"../../activities/publications/\" target=\"_top\">Publications</a></li>\n</ul>\n</li>\n<li class=\"nav-item\">\n<a class=\"nav-link\" href=\"../../research/\">Research</a>\n</li>\n<li class=\"nav-item\">\n<a class=\"nav-link\" href=\"../../software/\">Software</a>\n</li>\n<li class=\"nav-item\">\n<a class=\"nav-link\" href=\"../../events/\">Events</a>\n</li>\n<li class=\"nav-item\">\n<a class=\"nav-link\" href=\"../../resources/\">Resources</a>\n</li>\n</ul>\n</div>\n</nav>\n<body class=\"layout-reverse theme-base-db sidebar-overlay\" id=\"Site\">\n<!-- Wrap is the content to shift when toggling the sidebar. We wrap the\n      content to avoid any CSS collisions with our real content. -->\n<div class=\"wrap\">\n<div class=\"container content\">\n<div class=\"page container\">\n<h1 class=\"page-title\" style=\"text-align: left;\">The McGill Billboard Project</h1>\n<div style=\"margin-bottom: 25px\"></div>\n<p>Thank you for your interest in the McGill <em>Billboard</em> annotations! We are proud to announce a 2.0 release, with many improvements over the original, including new annotations, a number of corrections to errors in the existing annotations, and more accurate estimates of timing.</p>\n<p>In order to facilitate the best possible use of these data in the future, we have made them available legally under a CC0 license, but we ask that users follow scholarly norms in any public-facing work based on upon these data by citing the following ISMIR paper:</p>\n<ul>\n<li>John Ashley Burgoyne, Jonathan Wild, and Ichiro Fujinaga, \u2018An Expert Ground Truth Set for Audio Chord Recognition and Music Analysis\u2019, in <em>Proceedings of the 12th International Society for Music Information Retrieval Conference</em>, ed. Anssi Klapuri and Colby Leider (Miami, FL, 2011), pp. 633\u201338 <a href=\"http://ismir2011.ismir.net/papers/OS8-1.pdf\">[1]</a>;</li>\n</ul>\n<p>or Ashley Burgoyne\u2019s dissertation:</p>\n<ul>\n<li>John Ashley Burgoyne, \u2018Stochastic Processes and Database-Driven Musicology\u2019 (PhD diss., McGill University, Montr\u00e9al, Qu\u00e9bec, 2012) <a href=\"http://digitool.library.mcgill.ca/R/-?func=dbin-jump-full&amp;object_id=107704&amp;silo_library=GEN01\">[2]</a>.</li>\n</ul>\n<p>Please note that although the SALAMI project <a href=\"http://ismir2011.ismir.net/papers/PS4-14.pdf\">[3]</a> contributed the structural metadata to the McGill <em>Billboard</em> annotations, we gathered (and funded) the chord annotations independently from the SALAMI project, hence the distinct citation.</p>\n<h2 id=\"overview\">Overview</h2>\n<p>This release contains the annotations and audio features corresponding to the first 1000 entries from the random sample of <em>Billboard</em> chart slots as presented at ISMIR 2011, plus the additional 300 entries that were used to evaluate audio chord estimation for MIREX 2012. The set includes annotations and features for 890 slots, as we were unable to acquire audio for every entry in the sample, and comprises 740 distinct songs, as due to the nature of the sampling algorithm, some slots correspond to the same song. Training algorithms that assume independent, identically distributed data (as most do) should retain the duplicates. We will release annotations for the remaining 700 entries progressively over the next couple of years in order to ensure that there are unseen data available for evaluating algorithms at MIREX or related events.</p>\n<p>Since the original release, we have been able to complete a number of annotations that had originally encountered problems. Some of these new annotations are a closer match to the target sample than what was available at the time we released the original McGill <em>Billboard</em> annotations, and as such, a small number of entries refer to a different song in version 2.0 than they did in 1.x releases. We recommend that all users replace any previous editions of these annotations with version 2.0.</p>\n<p>This release is also split into multiple files, to reflect the needs of different users. Users may choose their preferred archive format (XZ, BZ2, or GZ) and extract all their relevant archives into a single directory. The result will be a <code class=\"language-plaintext highlighter-rouge\">McGill-Billboard</code> directory with a subdirectory for each annotated entry in the sample, each subdirectory containing the relevant annotations and features.\n            <br/>\n</p>\n<h2 id=\"index\">Index</h2>\n<p>Most users will want to download the index to the dataset:</p>\n<ul>\n<li><a href=\"https://www.dropbox.com/s/o0olz0uwl9z9stb/billboard-2.0-index.csv?dl=1\">billboard-2.0-index.csv</a></li>\n</ul>\n<p>The index is a CSV files with the following columns:</p>\n<ul>\n<li><strong>id</strong>, the index for the sample entry;</li>\n<li><strong>chart_date</strong>, the date of the chart for the entry;</li>\n<li><strong>target_rank</strong>, the desired rank on that chart;</li>\n<li><strong>actual_rank</strong>, the rank of the song actually annotated, which may be up to 2 ranks higher or lower than the target rank [1, 2];</li>\n<li><strong>title</strong>, the title of the song annotated;</li>\n<li><strong>artist</strong>, the name of the artist performing the song annotated;</li>\n<li><strong>peak_rank</strong>, the highest rank the song annotated ever achieved on the Billboard Hot 100; and</li>\n<li><strong>weeks_on_chart</strong>, the number of weeks the song annotated spent on the Billboard Hot 100 chart in total.</li>\n</ul>\n<p>Sample entries for which we were unable to obtain audio or an annotation also appear in the index, but with the <strong>id</strong>, <strong>chart_date</strong>, and <strong>target_rank</strong> columns exclusively.\n            <br/>\n</p>\n<h2 id=\"complete-annotations\">Complete Annotations</h2>\n<p>The complete annotations \u2013 chords, structure, instrumentation, and timing \u2013 are available from these links:</p>\n<ul>\n<li><a href=\"https://www.dropbox.com/s/p4xtixbvt4hw5c6/billboard-2.0-salami_chords.tar.xz?dl=1\">billboard-2.0-salami_chords.tar.xz</a></li>\n<li><a href=\"https://www.dropbox.com/s/g9yq0pafbfkbgxr/billboard-2.0-salami_chords.tar.bz2?dl=1\">billboard-2.0-salami_chords.tar.bz2</a></li>\n<li><a href=\"https://www.dropbox.com/s/2lvny9ves8kns4o/billboard-2.0-salami_chords.tar.gz?dl=1\">billboard-2.0-salami_chords.tar.gz</a></li>\n</ul>\n<p>The annotations files are named <code class=\"language-plaintext highlighter-rouge\">salami_chords.txt</code> to reflect the fact that they contain both SALAMI-style structural annotations and the McGill Billboard chord annotations.</p>\n<p>Each annotation begins with a header including the title of the song (prefixed by <code class=\"language-plaintext highlighter-rouge\"># title:</code>), the name of the artist (prefixed by <code class=\"language-plaintext highlighter-rouge\"># artist:</code>), the metre (prefixed by <code class=\"language-plaintext highlighter-rouge\"># metre:</code>), and the tonic pitch class of the opening key (prefixed by <code class=\"language-plaintext highlighter-rouge\"># tonic:</code>). Similar <code class=\"language-plaintext highlighter-rouge\">metre</code> and <code class=\"language-plaintext highlighter-rouge\">tonic</code> comments may also appear in the main body of the annotations, corresponding to changes of key or metre. In some cases, there is no obviously prevailing key, in which case the tonic pitch class is denoted <code class=\"language-plaintext highlighter-rouge\">?</code>.</p>\n<p>The main body of each annotation consists of a single line for each musical phrase or other sonic element at a comparable level of musical structure. Each line begins with a floating-point number denoting the timestamp of the beginning of the phrase (in seconds) followed by a tab character. There are special lines for <code class=\"language-plaintext highlighter-rouge\">silence</code> at the beginning and end of the audio file and a special line for the <code class=\"language-plaintext highlighter-rouge\">end</code> of the piece. The other lines continue with a comma-separated list of elements among the following.</p>\n<ul>\n<li><strong>Capital letters</strong>, possibly followed by an arbitrary number of primes (apostrophes), designate high-level musical structures. They appear at the beginning of each high-level musical segment and are presumed to continue until the next appearance of a capital letter. When two letters match, the two high-level segments are musically similar. Other than denoting similarity, the letters themselves have no intrinsic meaning, but for the letter Z. Z denotes non-musical passages in the audio such as noise or spoken words.</li>\n<li><strong>Plain text strings</strong> denote more traditional names for musical structures, e.g., verse, chorus, and bridge. The vocabulary was semi-restricted, but annotators had the freedom to use whatever terms they felt were most appropriate for unusual contexts.</li>\n<li><strong>Chord annotations</strong> appear as series of bars flanked by pipes (|). A phrase may by followed by an x and an integer, which means that the phrase is repeated that number of times. A phrase may also be followed by an arrow (-&gt;), which is a musicological hint that the phrase is musically elided into the following phrase.</li>\n<li><strong>Leading instruments</strong> are noted in songs where there is a notable deviation from the norm of a leading vocal throughout the entire song. They appear as text strings preceded by a left parenthesis (() in the segment where the instrument comes to prominence and as text strings succeeded by a right parenthesis ()) in the segment where that instrument fades from prominence. If an instrument is prominent for a single segment only, its name appears with both left and right parentheses.</li>\n</ul>\n<p>More detail on the structural annotations is available from Smith et al. <a href=\"http://ismir2011.ismir.net/papers/PS4-14.pdf\">[3]</a>. The McGill Billboard annotations replace the lower level of structural annotations from this reference (lowercase letters) with chord annotations.</p>\n<p>The chord annotations are simplified to the beat level. All chord symbols follow the standard that Harte et al. presented at ISMIR 2005 and used in MIREX ever since <a href=\"http://ismir2005.ismir.net/proceedings/1080.pdf\">[4]</a>, with a few additions to the shorthand to facilitate the richness of these annotations: <code class=\"language-plaintext highlighter-rouge\">1</code> for unharmonised bass notes, <code class=\"language-plaintext highlighter-rouge\">5</code> for power chords, and <code class=\"language-plaintext highlighter-rouge\">sus2</code>, <code class=\"language-plaintext highlighter-rouge\">maj11</code>, <code class=\"language-plaintext highlighter-rouge\">11</code>, <code class=\"language-plaintext highlighter-rouge\">min11</code>, <code class=\"language-plaintext highlighter-rouge\">maj13</code>, <code class=\"language-plaintext highlighter-rouge\">13</code>, and <code class=\"language-plaintext highlighter-rouge\">min13</code> for the corresponding chords in traditional jazz notation. An additional pseudo-chord type of <code class=\"language-plaintext highlighter-rouge\">1</code> denotes bass notes with no chord on top. To save space, repeated chords are denoted with a dot instead of the full chord name. To further save space, bars containing a single chord on all beats list the chord symbol only once; likewise, in quadruple metres (4/4 or 12/8), bars with only two chords and the change on the third beat list those two chords with no dots. For brief changes of metre, the metre may appear in parentheses at the beginning of the bar rather than as a full metre comment.</p>\n<p>Two non-chord symbols may appear within bars. For passages that were too musically elaborate to merit beat-level chord annotations, annotators sometimes filled the bar with an asterisk (*). For brief pauses of arbitrary length (often a single beat), annotators added a bar with the special annotation <code class=\"language-plaintext highlighter-rouge\">&amp;pause</code>.</p>\n<p>Bas de Haas has written Haskell tools for parsing and manipulating the McGill Billboard annotations <a href=\"http://www.cs.uu.nl/research/techreps/repo/CS-2012/2012-018.pdf\">[5]</a>, which are available from Hackage:</p>\n<ul>\n<li><a href=\"http://hackage.haskell.org/package/billboard-parser/\">http://hackage.haskell.org/package/billboard-parser/</a>\n<br/>\n</li>\n</ul>\n<h2 id=\"lab-files-mirex-style\">LAB Files (MIREX Style)</h2>\n<p>Users who are only interested in automatic chord recognition may prefer to download HTK-style LAB files for the chord annotations instead, which contain only onset times, offset times, and the chord labels, as used for the audio chord estimation task in MIREX:</p>\n<ul>\n<li><a href=\"https://www.dropbox.com/s/t390alzrkx0c9yt/billboard-2.0.1-lab.tar.xz?dl=1\">billboard-2.0.1-lab.tar.xz</a></li>\n<li><a href=\"https://www.dropbox.com/s/jk2nvyzdai3rbmz/billboard-2.0.1-lab.tar.bz2?dl=1\">billboard-2.0.1-lab.tar.bz2</a></li>\n<li><a href=\"https://www.dropbox.com/s/ep41gwy28vo3wxy/billboard-2.0.1-lab.tar.gz?dl=1\">billboard-2.0.1-lab.tar.gz</a></li>\n</ul>\n<p>Note that only the first chord of each phrase was time-aligned by a human. The timings for all other chords are linearly interpolated assuming a constant tempo for each phrase. This constant-tempo assumption is remarkably robust for the McGill Billboard sample: less than one percent of all possible eighth-note positions (tatums) in the sample are more then 10 percent faster or slower than the average tempo of the songs to which they belong <a href=\"http://www.cs.uu.nl/research/techreps/repo/CS-2012/2012-018.pdf\">[5]</a>.</p>\n<p>For convenience, we also have LAB files with chord labels simplified to the vocabularies that will be used for evaluating chord estimation in MIREX 2013:</p>\n<ul>\n<li><a href=\"https://www.dropbox.com/s/fg8lvy79o7etiyc/billboard-2.0.1-mirex.tar.xz?dl=1\">billboard-2.0.1-mirex.tar.xz</a></li>\n<li><a href=\"https://www.dropbox.com/s/1it49qcjx3l4dga/billboard-2.0.1-mirex.tar.bz2?dl=1\">billboard-2.0.1-mirex.tar.bz2</a></li>\n<li><a href=\"https://www.dropbox.com/s/f88s73bmivlvbiy/billboard-2.0.1-mirex.tar.gz?dl=1\">billboard-2.0.1-mirex.tar.gz</a>\n<br/>\n</li>\n</ul>\n<h2 id=\"audio-features\">Audio features</h2>\n<p>Although we cannot distribute the original audio due to copyright, we have two feature sets available. Users interested in chord recognition may want the non-negative-least-squares chroma vectors and tuning estimates from the <a href=\"http://www.isophonics.net/nnls-chroma\">Chordino VAMP plugin</a> <a href=\"http://ismir2010.ismir.net/proceedings/ismir2010-25.pdf\">[6]</a>:</p>\n<ul>\n<li><a href=\"https://www.dropbox.com/s/e9dm23vbawg9dsw/billboard-2.0-chordino.tar.xz?dl=1\">billboard-2.0-chordino.tar.xz</a></li>\n<li><a href=\"https://www.dropbox.com/s/w22zk8tpn0vffy7/billboard-2.0-chordino.tar.bz2?dl=1\">billboard-2.0-chordino.tar.bz2</a></li>\n<li><a href=\"https://www.dropbox.com/s/91ap0ho2e3507nm/billboard-2.0-chordino.tar.gz?dl=1\">billboard-2.0-chordino.tar.gz</a></li>\n</ul>\n<p>These archives contain <code class=\"language-plaintext highlighter-rouge\">bothchroma.csv</code> and <code class=\"language-plaintext highlighter-rouge\">tuning.csv</code> for each annotated single. We used the default settings for the plugin with the exception for a rolloff of 1 percent, the plugin authors\u2019 recommendation for pop music.</p>\n<p>Researchers of many kinds may find the Echo Nest features helpful. We have recomputed these with the <a href=\"http://developer.echonest.com/\">Echo Nest Analyzer</a> version 3.1.4:</p>\n<ul>\n<li><a href=\"https://www.dropbox.com/s/xwoh2ihudjk99s5/billboard-2.0-echonest.tar.xz?dl=1\">billboard-2.0-echonest.tar.xz</a></li>\n<li><a href=\"https://www.dropbox.com/s/flzygf12d1vqqpf/billboard-2.0-echonest.tar.bz2?dl=1\">billboard-2.0-echonest.tar.bz2</a></li>\n<li><a href=\"https://www.dropbox.com/s/8g8z6cgt6w1yosv/billboard-2.0-echonest.tar.gz?dl=1\">billboard-2.0-echonest.tar.gz</a></li>\n</ul>\n<p>If you are interested in audio features other than these, please contact us. So long as the features are non-invertible and the computational load is sane, we are happy to provide custom features upon request.\n            <br/>\n</p>\n<h2 id=\"contact\">Contact</h2>\n<p>Please e-mail any questions, comments, or bugs to Ashley Burgoyne at <a href=\"mailto:john.ashley.burgoyne@mail.mcgill.ca\">john.ashley.burgoyne@mail.mcgill.ca</a>.</p>\n<hr/>\n<ol>\n<li>\n<p>John Ashley Burgoyne, Jonathan Wild, and Ichiro Fujinaga, \u2018An Expert Ground Truth Set for Audio Chord Recognition and Music Analysis\u2019, in <em>Proceedings of the 12th International Society for Music Information Retrieval Conference</em>, ed. Anssi Klapuri and Colby Leider (Miami, FL, 2011), pp. 633\u201338, <a href=\"http://ismir2011.ismir.net/papers/OS8-1.pdf\">http://ismir2011.ismir.net/papers/OS8-1.pdf</a>.</p>\n</li>\n<li>\n<p>John Ashley Burgoyne, \u2018Stochastic Processes and Database-Driven Musicology\u2019 (PhD diss., McGill University, Montr\u00e9al, Qu\u00e9bec, 2012), <a href=\"http://digitool.Library.McGill.CA:80/R/-?func=dbin-jump-full&amp;object_id=107704&amp;silo_library=GEN01\">http://digitool.Library.McGill.CA:80/R/-?func=dbin-jump-full&amp;object_id=107704&amp;silo_library=GEN01</a>.</p>\n</li>\n<li>\n<p>Jordan B. L. Smith, J. Ashley Burgoyne, Ichiro Fujinaga, David De Roure, and J. Stephen Downie, \u2018Design and Creation of a Large-Scale Database of Structural Annotations\u2019, in <em>Proceedings of the 12th International Society for Music Information Retrieval Conference</em>, ed. Anssi Klapuri and Colby Leider (Miami, FL, 2011), pp. 55\u201360, <a href=\"http://ismir2011.ismir.net/papers/PS4-14.pdf\">http://ismir2011.ismir.net/papers/PS4-14.pdf</a>.</p>\n</li>\n<li>\n<p>Christopher A. Harte, Mark B. Sandler, Samer A. Abdallah, and Emilia G\u00f3mez, \u2018Symbolic Representation of Musical Chords: A Proposed Syntax for Text Annotations\u2019, in <em>Proceedings of the 6th International Conference on Music Information Retrieval</em>, ed. Joshua D. Reiss and Geraint A. Wiggins (London, England, 2005), pp. 66\u201371, <a href=\"http://ismir2005.ismir.net/proceedings/1080.pdf\">http://ismir2005.ismir.net/proceedings/1080.pdf</a>.</p>\n</li>\n<li>\n<p>W. Bas de Haas and John Ashley Burgoyne, \u2018Parsing the Billboard chord transcriptions\u2019 (Technical report UU-CS-2012-18, Utrecht University, the Netherlands, 2012), <a href=\"http://www.cs.uu.nl/research/techreps/repo/CS-2012/2012-018.pdf\">http://www.cs.uu.nl/research/techreps/repo/CS-2012/2012-018.pdf</a>.</p>\n</li>\n<li>\n<p>Matthias Mauch and Simon Dixon, \u2018Approximate Note Transcription for the Improved Identification of Difficult Chords\u2019, in Proceedings of the <em>11th International Society for Music Information Retrieval Conference</em>, ed. J. Stephen Downie and Remco C. Veltkamp (Utrecht, the Netherlands, 2010), pp. 135\u201340, <a href=\"http://ismir2010.ismir.net/proceedings/ismir2010-25.pdf\">http://ismir2010.ismir.net/proceedings/ismir2010-25.pdf</a>.</p>\n</li>\n</ol>\n<hr/>\n<p><img alt=\"Public domain\" src=\"../../assets/public_domain.png\" title=\"Public domain\"/>\n<br/>\n            To the extent possible under law, the DDMAL has waived all copyright and related or neighbouring rights to the McGill <em>Billboard</em> annotations. This work is published from Canada.\n          </p>\n</div>\n<br/>\n<br/>\n</div>\n</div>\n<label class=\"sidebar-toggle\" for=\"sidebar-checkbox\"></label>\n<footer id=\"sticky\">\n<div class=\"footer-img-wrap\">\n<img alt=\"\" class=\"mcgill-img-footer\" src=\"../../assets/schulich_logo.png\"/>\n<img alt=\"\" class=\"ddmal-img-footer\" src=\"../../assets/Ddmal_logo_transp-bg_no-border_1600w.png\"/>\n</div>\n</footer>\n\n\n\n\n</body>\n</html>"}